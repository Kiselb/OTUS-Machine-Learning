{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fine tuning –º–æ–¥–µ–ª–∏ RuBert –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö RuCola "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–¥—Ä—É–≥ —Ä–µ—à–µ—Ç–∫–∞ –±–µ–∑–∑–≤—É—á–Ω–æ –ø–æ–µ—Ö–∞–ª–∞ –≤ —Å—Ç–æ—Ä–æ–Ω—É, –∏ –Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–≠—Ç–∏–º –ª–µ—Ç–æ–º –Ω–µ –Ω–∏–∫—É–¥–∞ –µ–∑–¥–∏–ª–∏.</td>\n",
       "      <td>0</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–¢–æ–ª—å–∫–æ –ò–≤–∞–Ω –≤—ã—Ä–∞–∑–∏–ª –∫–∞–∫—É—é –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ –≥–æ—Ç–æ–≤–Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–¢–µ–ø–µ—Ä—å —Ç—ã –≤–∏–¥–∏—à—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏, –∫–∞–∫ —Ç—É—Ç...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–ù–∞ –ø–æ–≤–µ—Ä–∫—É –≤—Å—è —Ç–µ–æ—Ä–∏—è –æ–∫–∞–∑–∞–ª–∞—Å—å –ø–æ–ª–Ω–æ–π —á–µ–ø—É—Ö–æ–π.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>7864</td>\n",
       "      <td>–£—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ –±—ã–ª–æ –≤–≤–µ–¥–µ–Ω–æ –≤ –¥–µ–π—Å—Ç–≤–∏–µ.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>7865</td>\n",
       "      <td>–ö–æ–Ω–µ—á–Ω–æ, –ø—Ä–æ—Ç–∏–≤ —Ç–∞–∫–æ–π —Å–∏—Å—Ç–µ–º—ã —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–µ—à–∏—Ç–µ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>7866</td>\n",
       "      <td>–°–∏–º–ø—Ç–æ–º–æ–≤ –±–æ–ª–µ–∑–Ω–∏ –Ω–µ –∏—Å—á–µ–∑–ª–æ.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>7867</td>\n",
       "      <td>–ü–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É –±–æ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –¥...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>7868</td>\n",
       "      <td>–ì–æ–≤–æ—Ä—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –∫–∞—Ä—Ç–∏–Ω–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ò–≤–∞–Ω–æ–≤–∞...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7869 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence  acceptable  \\\n",
       "0        0  –í–¥—Ä—É–≥ —Ä–µ—à–µ—Ç–∫–∞ –±–µ–∑–∑–≤—É—á–Ω–æ –ø–æ–µ—Ö–∞–ª–∞ –≤ —Å—Ç–æ—Ä–æ–Ω—É, –∏ –Ω...           1   \n",
       "1        1                       –≠—Ç–∏–º –ª–µ—Ç–æ–º –Ω–µ –Ω–∏–∫—É–¥–∞ –µ–∑–¥–∏–ª–∏.           0   \n",
       "2        2  –¢–æ–ª—å–∫–æ –ò–≤–∞–Ω –≤—ã—Ä–∞–∑–∏–ª –∫–∞–∫—É—é –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ –≥–æ—Ç–æ–≤–Ω...           1   \n",
       "3        3  –¢–µ–ø–µ—Ä—å —Ç—ã –≤–∏–¥–∏—à—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏, –∫–∞–∫ —Ç—É—Ç...           1   \n",
       "4        4    –ù–∞ –ø–æ–≤–µ—Ä–∫—É –≤—Å—è —Ç–µ–æ—Ä–∏—è –æ–∫–∞–∑–∞–ª–∞—Å—å –ø–æ–ª–Ω–æ–π —á–µ–ø—É—Ö–æ–π.           1   \n",
       "...    ...                                                ...         ...   \n",
       "7864  7864              –£—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ –±—ã–ª–æ –≤–≤–µ–¥–µ–Ω–æ –≤ –¥–µ–π—Å—Ç–≤–∏–µ.           0   \n",
       "7865  7865  –ö–æ–Ω–µ—á–Ω–æ, –ø—Ä–æ—Ç–∏–≤ —Ç–∞–∫–æ–π —Å–∏—Å—Ç–µ–º—ã —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–µ—à–∏—Ç–µ...           0   \n",
       "7866  7866                      –°–∏–º–ø—Ç–æ–º–æ–≤ –±–æ–ª–µ–∑–Ω–∏ –Ω–µ –∏—Å—á–µ–∑–ª–æ.           0   \n",
       "7867  7867  –ü–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É –±–æ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –¥...           0   \n",
       "7868  7868  –ì–æ–≤–æ—Ä—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –∫–∞—Ä—Ç–∏–Ω–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ò–≤–∞–Ω–æ–≤–∞...           0   \n",
       "\n",
       "     error_type detailed_source  \n",
       "0             0   Paducheva2004  \n",
       "1        Syntax         Rusgram  \n",
       "2             0   Paducheva2013  \n",
       "3             0   Paducheva2010  \n",
       "4             0   Paducheva2010  \n",
       "...         ...             ...  \n",
       "7864  Semantics   Paducheva2004  \n",
       "7865  Semantics   Paducheva2013  \n",
       "7866  Semantics   Paducheva2013  \n",
       "7867  Semantics         Rusgram  \n",
       "7868  Semantics   Paducheva2013  \n",
       "\n",
       "[7869 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"in_domain_train.csv\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–ò–≤–∞–Ω –≤—á–µ—Ä–∞ –Ω–µ –ø–æ–∑–≤–æ–Ω–∏–ª.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–£ –º–Ω–æ–≥–∏—Ö —Ç—É—Ä–∏—Å—Ç–æ–≤, –∫—Ç–æ –ø–æ—Å–µ—â–∞—é—Ç –ö–µ–º–µ—Ä –≤–µ—Å–Ω–æ–π, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>USE8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–õ–µ—Å–Ω—ã–µ –∑–∞–ø–∞—Ö–∏ –Ω–∞–±–µ–≥–∞–ª–∏ –≤–æ–ª–Ω–∞–º–∏; –≤ –Ω–∏—Ö —Å–º–µ—à–∞–ª–æ—Å...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>USE5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–í—á–µ—Ä–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –∏–º–µ–ª –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –±–µ—Å–µ–¥—É —Å –∞–Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Seliverstova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–ö–æ–ª–ª–µ–≥–∞ —Ç–∞–∫ –∏ –Ω–µ –ø—Ä–∏–∑–Ω–∞–ª –≤–∏–Ω—É –∑–∞ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—É –ø–µ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Testelets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>978</td>\n",
       "      <td>–ú—ã—Å–ª–∏ –æ—Ç–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ –≤—Å—è–∫–æ–º –ø—Ä–µ–¥...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>979</td>\n",
       "      <td>–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–π, —á—Ç–æ —Å—É–¥—å—é –ø—Ä–∏–≤–ª–µ–∫–∞—é...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>980</td>\n",
       "      <td>–•–æ—Ä–æ—à–æ, —á—Ç–æ –æ–Ω –∫—É–ø–∏–ª —á—Ç–æ-–Ω–∏–±—É–¥—å.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>–ï—Å–ª–∏ –±—ã —è –Ω–µ –ø–æ—Ç–µ—Ä—è–ª –æ—á–∫–æ–≤, –Ω–µ –ø—Ä–∏—à–ª–æ—Å—å –±—ã –ø–æ–∫...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982</td>\n",
       "      <td>–ù–∞ –ú–∞—Ä—Å–µ –µ—Å—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ (–∫–∞–∫–∏–µ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ)...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>983 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           sentence  acceptable  \\\n",
       "0      0                            –ò–≤–∞–Ω –≤—á–µ—Ä–∞ –Ω–µ –ø–æ–∑–≤–æ–Ω–∏–ª.           1   \n",
       "1      1  –£ –º–Ω–æ–≥–∏—Ö —Ç—É—Ä–∏—Å—Ç–æ–≤, –∫—Ç–æ –ø–æ—Å–µ—â–∞—é—Ç –ö–µ–º–µ—Ä –≤–µ—Å–Ω–æ–π, ...           0   \n",
       "2      2  –õ–µ—Å–Ω—ã–µ –∑–∞–ø–∞—Ö–∏ –Ω–∞–±–µ–≥–∞–ª–∏ –≤–æ–ª–Ω–∞–º–∏; –≤ –Ω–∏—Ö —Å–º–µ—à–∞–ª–æ—Å...           1   \n",
       "3      3  –í—á–µ—Ä–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –∏–º–µ–ª –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –±–µ—Å–µ–¥—É —Å –∞–Ω...           1   \n",
       "4      4  –ö–æ–ª–ª–µ–≥–∞ —Ç–∞–∫ –∏ –Ω–µ –ø—Ä–∏–∑–Ω–∞–ª –≤–∏–Ω—É –∑–∞ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—É –ø–µ...           1   \n",
       "..   ...                                                ...         ...   \n",
       "978  978  –ú—ã—Å–ª–∏ –æ—Ç–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ –≤—Å—è–∫–æ–º –ø—Ä–µ–¥...           0   \n",
       "979  979  –ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–π, —á—Ç–æ —Å—É–¥—å—é –ø—Ä–∏–≤–ª–µ–∫–∞—é...           0   \n",
       "980  980                   –•–æ—Ä–æ—à–æ, —á—Ç–æ –æ–Ω –∫—É–ø–∏–ª —á—Ç–æ-–Ω–∏–±—É–¥—å.           0   \n",
       "981  981  –ï—Å–ª–∏ –±—ã —è –Ω–µ –ø–æ—Ç–µ—Ä—è–ª –æ—á–∫–æ–≤, –Ω–µ –ø—Ä–∏—à–ª–æ—Å—å –±—ã –ø–æ–∫...           0   \n",
       "982  982  –ù–∞ –ú–∞—Ä—Å–µ –µ—Å—Ç—å –∫–∞–∫–∏–µ-–ª–∏–±–æ (–∫–∞–∫–∏–µ –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ)...           0   \n",
       "\n",
       "    error_type detailed_source  \n",
       "0            0   Paducheva2013  \n",
       "1       Syntax            USE8  \n",
       "2            0            USE5  \n",
       "3            0    Seliverstova  \n",
       "4            0       Testelets  \n",
       "..         ...             ...  \n",
       "978  Semantics   Paducheva2013  \n",
       "979  Semantics   Paducheva2013  \n",
       "980  Semantics         Rusgram  \n",
       "981  Semantics   Paducheva2013  \n",
       "982  Semantics         Rusgram  \n",
       "\n",
       "[983 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev = pd.read_csv(\"in_domain_dev.csv\")\n",
    "data_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence', 'label', 'error_type', 'detailed_source'],\n",
       "        num_rows: 6295\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence', 'label', 'error_type', 'detailed_source'],\n",
       "        num_rows: 1574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data_train).train_test_split(test_size=0.2, seed=1)\n",
    "dataset = dataset.rename_column('acceptable', 'label')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'ai-forever/ruBert-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6295/6295 [00:00<00:00, 13625.03 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [00:00<00:00, 15739.11 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6295\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenized = dataset.map(lambda x: tokenizer(x['sentence'], truncation=True, max_length=512), batched=True, remove_columns=['id', 'sentence', 'error_type', 'detailed_source'])\n",
    "data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(data_tokenized['train'], shuffle=True, batch_size=4, collate_fn=collator)\n",
    "val_dataloader = DataLoader(data_tokenized['test'], shuffle=False, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(base_model, num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "loss: 0.54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [07:44<00:00,  3.39it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:10<00:00, 38.41it/s]\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [07:54<15:49, 474.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent train loss 0.5432492765784264 eval loss 0.5517015452993098 accuracy 0.7465057179161372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [08:53<00:00,  2.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:11<00:00, 33.25it/s]\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [16:59<08:36, 516.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent train loss 0.5714918261766434 eval loss 0.5377188452004176 accuracy 0.7592121982210928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [09:13<00:00,  2.84it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:11<00:00, 34.12it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [26:24<00:00, 528.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent train loss 0.4805538737773895 eval loss 0.5404081200131305 accuracy 0.7649301143583227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in trange(3):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    model.train()\n",
    "    for i, batch in enumerate(pbar):\n",
    "        out = model(**batch.to(model.device))\n",
    "        out.loss.backward()\n",
    "        if i % 1 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        losses.append(out.loss.item())\n",
    "        pbar.set_description(f'loss: {np.mean(losses[-100:]):2.2f}')\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_preds = []\n",
    "    eval_targets = []\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "                out = model(**batch.to(model.device))\n",
    "        eval_losses.append(out.loss.item())\n",
    "        eval_preds.extend(out.logits.argmax(1).tolist())\n",
    "        eval_targets.extend(batch['labels'].tolist())\n",
    "    print('recent train loss', np.mean(losses[-100:]), 'eval loss', np.mean(eval_losses), 'accuracy', np.mean(np.array(eval_targets) == eval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'sentence', 'label', 'error_type', 'detailed_source'],\n",
       "    num_rows: 983\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev = Dataset.from_pandas(data_dev)\n",
    "dataset_dev = dataset_dev.rename_column('acceptable', 'label')\n",
    "dataset_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 983/983 [00:00<00:00, 23986.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 983\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tokenized = dataset_dev.map(lambda x: tokenizer(x['sentence'], truncation=True, max_length=512), batched=True, remove_columns=['id', 'sentence', 'error_type', 'detailed_source'])\n",
    "test_data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data_tokenized, shuffle=True, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 246/246 [00:07<00:00, 33.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent train loss 0.4805538737773895 eval loss 0.5220698794213737 accuracy 0.7639877924720244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_losses = []\n",
    "eval_preds = []\n",
    "eval_targets = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "            out = model(**batch.to(model.device))\n",
    "    eval_losses.append(out.loss.item())\n",
    "    eval_preds.extend(out.logits.argmax(1).tolist())\n",
    "    eval_targets.extend(batch['labels'].tolist())\n",
    "print('recent train loss', np.mean(losses[-100:]), 'eval loss', np.mean(eval_losses), 'accuracy', np.mean(np.array(eval_targets) == eval_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RuGPT3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç—å—è](https://cloud.ru/ru/datahub/rugpt3family/rugpt-3-large)\n",
    "\n",
    "[–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/Generate_text_with_RuGPTs_HF.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name_or_path = \"ai-forever/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–≤–µ—Ä–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á –ü—É—à–∫–∏–Ω —Ä–æ–¥–∏–ª—Å—è –≤ \n",
      "1799 –≥–æ–¥—É –≤ –ú–æ—Å–∫–≤–µ.\n",
      "\n",
      "–í 1820 –≥–æ–¥—É,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –°–µ—Ä–≥–µ–µ–≤–∏—á –ü—É—à–∫–∏–Ω —Ä–æ–¥–∏–ª—Å—è –≤ \"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "out = model.generate(input_ids.cuda())\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–º–ø—Ç–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ –∫—Ä—É—Ç–æ\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –í —á–µ–º –æ—Ç–ª–∏—á–∏–µ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –≤ –±—Ä–∞—É–∑–µ—Ä–µ –Ω–µ –±—ã–ª–æ —Ä–µ–∫–ª–∞–º—ã\n",
      " –¢–µ–∫—Å—Ç: –ö–∞–∫ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫\n"
     ]
    }
   ],
   "source": [
    "text = \"–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\\n –¢–µ–∫—Å—Ç: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–æ –∫—Ä—É—Ç–æ\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "out = model.generate(input_ids.cuda(), max_new_tokens=200, )\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∞ —Å–µ–Ω–æ –ª–µ–∂–∞–ª–∞\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∞ —Å–µ–Ω–æ –ª–µ–∂–∞–ª–∞\n"
     ]
    }
   ],
   "source": [
    "text = \"–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\\n –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∞ —Å–µ–Ω–æ –ª–µ–∂–∞–ª–∞\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "out = model.generate(input_ids.cuda(), max_new_tokens=200, )\n",
    "generated_text = list(map(tokenizer.decode, out))[0][0:135]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∞ —Å–µ–Ω–æ –ª–µ–∂–∞–ª–∞\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∏ —Å—Ç–∞—è —Ç—Ä–∞–≤–∏–ª–∏ –≤–æ–ª–∫–∏\n",
      " –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π\n",
      " –¢–µ–∫—Å—Ç\n"
     ]
    }
   ],
   "source": [
    "text = \"–¢–µ–∫—Å—Ç: –®–ª–∞ —Å–æ–±–∞–∫–∞ –ø–æ —Ä–æ—è–ª—é.\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –¥–æ–ø—É—Å—Ç–∏–º—ã–π\\n –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∞ —Å–µ–Ω–æ –ª–µ–∂–∞–ª–∞\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π\\n –¢–µ–∫—Å—Ç: –°–æ–±–∞–∫–∏ —Å—Ç–∞—è —Ç—Ä–∞–≤–∏–ª–∏ –≤–æ–ª–∫–∏\\n –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n",
    "out = model.generate(input_ids.cuda(), max_new_tokens=200, )\n",
    "generated_text = list(map(tokenizer.decode, out))[0][0:180]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RuT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "from shutil import rmtree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>error_type</th>\n",
       "      <th>detailed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–¥—Ä—É–≥ —Ä–µ—à–µ—Ç–∫–∞ –±–µ–∑–∑–≤—É—á–Ω–æ –ø–æ–µ—Ö–∞–ª–∞ –≤ —Å—Ç–æ—Ä–æ–Ω—É, –∏ –Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–≠—Ç–∏–º –ª–µ—Ç–æ–º –Ω–µ –Ω–∏–∫—É–¥–∞ –µ–∑–¥–∏–ª–∏.</td>\n",
       "      <td>0</td>\n",
       "      <td>Syntax</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–¢–æ–ª—å–∫–æ –ò–≤–∞–Ω –≤—ã—Ä–∞–∑–∏–ª –∫–∞–∫—É—é –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ –≥–æ—Ç–æ–≤–Ω...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–¢–µ–ø–µ—Ä—å —Ç—ã –≤–∏–¥–∏—à—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏, –∫–∞–∫ —Ç—É—Ç...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–ù–∞ –ø–æ–≤–µ—Ä–∫—É –≤—Å—è —Ç–µ–æ—Ä–∏—è –æ–∫–∞–∑–∞–ª–∞—Å—å –ø–æ–ª–Ω–æ–π —á–µ–ø—É—Ö–æ–π.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paducheva2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>7864</td>\n",
       "      <td>–£—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ –±—ã–ª–æ –≤–≤–µ–¥–µ–Ω–æ –≤ –¥–µ–π—Å—Ç–≤–∏–µ.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>7865</td>\n",
       "      <td>–ö–æ–Ω–µ—á–Ω–æ, –ø—Ä–æ—Ç–∏–≤ —Ç–∞–∫–æ–π —Å–∏—Å—Ç–µ–º—ã —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–µ—à–∏—Ç–µ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7866</th>\n",
       "      <td>7866</td>\n",
       "      <td>–°–∏–º–ø—Ç–æ–º–æ–≤ –±–æ–ª–µ–∑–Ω–∏ –Ω–µ –∏—Å—á–µ–∑–ª–æ.</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>7867</td>\n",
       "      <td>–ü–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É –±–æ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –¥...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Rusgram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>7868</td>\n",
       "      <td>–ì–æ–≤–æ—Ä—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –∫–∞—Ä—Ç–∏–Ω–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ò–≤–∞–Ω–æ–≤–∞...</td>\n",
       "      <td>0</td>\n",
       "      <td>Semantics</td>\n",
       "      <td>Paducheva2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7869 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence  acceptable  \\\n",
       "0        0  –í–¥—Ä—É–≥ —Ä–µ—à–µ—Ç–∫–∞ –±–µ–∑–∑–≤—É—á–Ω–æ –ø–æ–µ—Ö–∞–ª–∞ –≤ —Å—Ç–æ—Ä–æ–Ω—É, –∏ –Ω...           1   \n",
       "1        1                       –≠—Ç–∏–º –ª–µ—Ç–æ–º –Ω–µ –Ω–∏–∫—É–¥–∞ –µ–∑–¥–∏–ª–∏.           0   \n",
       "2        2  –¢–æ–ª—å–∫–æ –ò–≤–∞–Ω –≤—ã—Ä–∞–∑–∏–ª –∫–∞–∫—É—é –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ –≥–æ—Ç–æ–≤–Ω...           1   \n",
       "3        3  –¢–µ–ø–µ—Ä—å —Ç—ã –≤–∏–¥–∏—à—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –≥–ª–∞–∑–∞–º–∏, –∫–∞–∫ —Ç—É—Ç...           1   \n",
       "4        4    –ù–∞ –ø–æ–≤–µ—Ä–∫—É –≤—Å—è —Ç–µ–æ—Ä–∏—è –æ–∫–∞–∑–∞–ª–∞—Å—å –ø–æ–ª–Ω–æ–π —á–µ–ø—É—Ö–æ–π.           1   \n",
       "...    ...                                                ...         ...   \n",
       "7864  7864              –£—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ –±—ã–ª–æ –≤–≤–µ–¥–µ–Ω–æ –≤ –¥–µ–π—Å—Ç–≤–∏–µ.           0   \n",
       "7865  7865  –ö–æ–Ω–µ—á–Ω–æ, –ø—Ä–æ—Ç–∏–≤ —Ç–∞–∫–æ–π —Å–∏—Å—Ç–µ–º—ã —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π —Ä–µ—à–∏—Ç–µ...           0   \n",
       "7866  7866                      –°–∏–º–ø—Ç–æ–º–æ–≤ –±–æ–ª–µ–∑–Ω–∏ –Ω–µ –∏—Å—á–µ–∑–ª–æ.           0   \n",
       "7867  7867  –ü–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É –±–æ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è –¥...           0   \n",
       "7868  7868  –ì–æ–≤–æ—Ä—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ –∫–∞—Ä—Ç–∏–Ω–µ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ò–≤–∞–Ω–æ–≤–∞...           0   \n",
       "\n",
       "     error_type detailed_source  \n",
       "0             0   Paducheva2004  \n",
       "1        Syntax         Rusgram  \n",
       "2             0   Paducheva2013  \n",
       "3             0   Paducheva2010  \n",
       "4             0   Paducheva2010  \n",
       "...         ...             ...  \n",
       "7864  Semantics   Paducheva2004  \n",
       "7865  Semantics   Paducheva2013  \n",
       "7866  Semantics   Paducheva2013  \n",
       "7867  Semantics         Rusgram  \n",
       "7868  Semantics   Paducheva2013  \n",
       "\n",
       "[7869 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"in_domain_train.csv\")\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence', 'label', 'error_type', 'detailed_source'],\n",
       "        num_rows: 6295\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence', 'label', 'error_type', 'detailed_source'],\n",
       "        num_rows: 1574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data_train).train_test_split(test_size=0.2, seed=1)\n",
    "dataset = dataset.rename_column('acceptable', 'label')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"sberbank-ai/ruT5-base\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/6295 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6295/6295 [00:00<00:00, 7467.35 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1574/1574 [00:00<00:00, 7791.95 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6295\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenized = dataset.map(lambda x: tokenizer(x['sentence'], truncation=True, max_length=512), batched=True, remove_columns=['id', 'sentence', 'error_type', 'detailed_source'])\n",
    "data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(data_tokenized['train'], shuffle=True, batch_size=4, collate_fn=collator)\n",
    "val_dataloader = DataLoader(data_tokenized['test'], shuffle=False, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MVKiselev\\AppData\\Local\\Temp\\ipykernel_31216\\1872012557.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  ACCURACY = load_metric(\"accuracy\", keep_in_memory=True, trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "ACCURACY = load_metric(\"accuracy\", keep_in_memory=True, trust_remote_code=True)\n",
    "MCC = load_metric(\"matthews_correlation\", keep_in_memory=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEEDS = 10\n",
    "N_EPOCHS = 20\n",
    "LR_VALUES = (1e-4, 1e-3)\n",
    "DECAY_VALUES = (0, 1e-4)\n",
    "BATCH_SIZES = (128,)\n",
    "\n",
    "POS_LABEL = \"yes\"\n",
    "NEG_LABEL = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p, tokenizer):\n",
    "    string_preds = tokenizer.batch_decode(p.predictions, skip_special_tokens=True)\n",
    "    int_preds = [1 if prediction == POS_LABEL else 0 for prediction in string_preds]\n",
    "\n",
    "    labels = np.where(p.label_ids != -100, p.label_ids, tokenizer.pad_token_id)\n",
    "    string_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    int_labels = []\n",
    "\n",
    "    for string_label in string_labels:\n",
    "        if string_label == POS_LABEL:\n",
    "            int_labels.append(1)\n",
    "        elif string_label == NEG_LABEL or string_label == \"\":  # second case accounts for test data\n",
    "            int_labels.append(0)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    acc_result = ACCURACY.compute(predictions=int_preds, references=int_labels)\n",
    "    mcc_result = MCC.compute(predictions=int_preds, references=int_labels)\n",
    "\n",
    "    result = {\"accuracy\": acc_result[\"accuracy\"], \"mcc\": mcc_result[\"matthews_correlation\"]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i, learning_rate in enumerate(LR_VALUES):\n",
    "        for j, weight_decay in enumerate(DECAY_VALUES):\n",
    "            for k, batch_size in enumerate(BATCH_SIZES):\n",
    "                for seed in range(N_SEEDS):\n",
    "                    \n",
    "                    model = model = T5ForConditionalGeneration.from_pretrained(\"sberbank-ai/ruT5-base\")\n",
    "\n",
    "                    run_base_dir = f\"sberbank-ai_ruT5-base_{learning_rate}_{weight_decay}_{batch_size}\"\n",
    "\n",
    "                    training_args = Seq2SeqTrainingArguments(\n",
    "                        output_dir=f\"checkpoints/{run_base_dir}\",\n",
    "                        overwrite_output_dir=True,\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=learning_rate,\n",
    "                        weight_decay=weight_decay,\n",
    "                        num_train_epochs=N_EPOCHS,\n",
    "                        lr_scheduler_type=\"constant\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        save_total_limit=1,\n",
    "                        seed=seed,\n",
    "                        fp16=True,\n",
    "                        dataloader_num_workers=4,\n",
    "                        group_by_length=True,\n",
    "                        report_to=\"none\",\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model=\"eval_mcc\",\n",
    "                        optim=\"adafactor\",\n",
    "                        predict_with_generate=True,\n",
    "                    )\n",
    "\n",
    "                    trainer = Seq2SeqTrainer(\n",
    "                        model=model,\n",
    "                        args=training_args,\n",
    "                        train_dataset=data_tokenized[\"train\"],\n",
    "                        eval_dataset=data_tokenized[\"test\"],\n",
    "                        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "                        tokenizer=tokenizer,\n",
    "                        data_collator=data_collator,\n",
    "                    )\n",
    "\n",
    "                    train_result = trainer.train()\n",
    "                    print(f\"{run_base_dir}_{seed}\")\n",
    "                    print(\"train\", train_result.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py\", line 617, in __call__\n    max_label_length = max(len(l) for l in labels) if not max_padding else self.max_length\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py\", line 617, in <genexpr>\n    max_label_length = max(len(l) for l in labels) if not max_padding else self.max_length\n                           ^^^^^^\nTypeError: object of type 'int' has no len()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 44\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m     12\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_base_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     predict_with_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     36\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_base_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_result\u001b[38;5;241m.\u001b[39mmetrics)\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2179\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py\", line 617, in __call__\n    max_label_length = max(len(l) for l in labels) if not max_padding else self.max_length\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\MVKiselev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\data_collator.py\", line 617, in <genexpr>\n    max_label_length = max(len(l) for l in labels) if not max_padding else self.max_length\n                           ^^^^^^\nTypeError: object of type 'int' has no len()\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
